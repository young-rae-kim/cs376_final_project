{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db099f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: albumentations in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (1.21.6)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (0.19.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (4.5.5.64)\n",
      "Requirement already satisfied: scipy in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (1.8.0)\n",
      "Requirement already satisfied: PyYAML in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (4.1.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.19.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.8)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2022.5.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: tqdm in /Users/young-rae-kim/opt/anaconda3/lib/python3.9/site-packages (4.64.0)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries for data_loader\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install albumentations\n",
    "!pip install torchvision\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307d1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataLoader and corresponding libraries\n",
    "import pandas\n",
    "import torchvision.transforms as TT\n",
    "# import albumentations as T\n",
    "# import albumentations.augmentations.transforms as T_transforms\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision import utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88dc1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for tensors\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824ee728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bar construction\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18c7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Datatypes and Devices (from Assignment 2)\n",
    "dtype = torch.float\n",
    "ltype = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cde023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for K-Fold Cross Validation\n",
    "N = 5\n",
    "seed = 42\n",
    "\n",
    "# Directories for Data\n",
    "FF1010_Path = './data/'\n",
    "AudioImage_Path = './image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ff2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call StratifiedKFold object\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3a6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for K-Fold Cross Validation (ff1010)\n",
    "ff1010_csv = pandas.read_csv(FF1010_Path + 'metadata.csv')\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 0, 'filepath'] = \\\n",
    "    FF1010_Path + 'nocall/' + ff1010_csv.query('hasbird == 0')['filename'] + '.npy'\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 1, 'filepath'] = \\\n",
    "    FF1010_Path + 'bird/' + ff1010_csv.query('hasbird == 1')['filename'] + '.npy'\n",
    "\n",
    "ff1010_csv = ff1010_csv.dropna()\n",
    "ff1010_csv = ff1010_csv.reset_index(drop=True)\n",
    "\n",
    "# Add 'fold' attribute for dataset classification\n",
    "ff1010_dataframe = ff1010_csv.copy()\n",
    "for n, (_, nth_groups) in enumerate(\n",
    "    skf.split(ff1010_dataframe, ff1010_dataframe['hasbird'])):\n",
    "    ff1010_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b45549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify dataframe for K-Fold Cross Validation (birdclef2021)\n",
    "# birdclef_csv = pandas.read_csv(AudioImage_Path + 'metadata.csv')\n",
    "# birdclef_csv.loc[birdclef_csv['label_id'] >= 0,'filepath'] = \\\n",
    "#     AudioImage_Path + birdclef_csv.query('label_id >= 0')['primary_label'] + '/' + \\\n",
    "#     birdclef_csv.query('label_id >= 0')['filename'] + '.npy'\n",
    "\n",
    "# birdclef_csv = birdclef_csv.dropna()\n",
    "# birdclef_csv = birdclef_csv.reset_index(drop=True)\n",
    "\n",
    "# # Add 'fold' attribute for dataset classification\n",
    "# birdclef_dataframe = birdclef_csv.copy()\n",
    "# for n, (_, nth_groups) in enumerate(\n",
    "#     skf.split(birdclef_dataframe, birdclef_dataframe['label_id'])):\n",
    "#     birdclef_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d909d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for training \n",
    "ff1010_batch = 32\n",
    "birdclef_batch = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5213b97",
   "metadata": {},
   "source": [
    "Model 1 (No-call detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for ff1010 dataset\n",
    "class FF1010(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='hasbird'):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.train_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.RandomHorizontalFlip(p=0.5),\n",
    "            TT.RandomVerticalFlip(p=0.5),\n",
    "#             T_transforms.ImageCompression(p=0.5, \n",
    "#                 compression_type=T_transforms.ImageCompression.ImageCompressionType.JPEG),\n",
    "#             T_transforms.ImageCompression(p=0.5, \n",
    "#                 compression_type=T_transforms.ImageCompression.ImageCompressionType.WEBP),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "        self.val_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.process == 'train':\n",
    "            source = self.train_transform(Image.fromarray(source))\n",
    "        elif self.process == 'valid':\n",
    "            ource = self.val_transform(Image.fromarray(source))\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c650ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Train loop for nocall detector\n",
    "def nocall_train(train_dataframe, val_dataframe):  \n",
    "    train_data = FF1010(train_dataframe, process='train', labels='hasbird')\n",
    "    val_data = FF1010(val_dataframe, process='valid', labels='hasbird')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    \n",
    "    # Test for loaders\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 training 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        sleep(0.01)\n",
    "        \n",
    "    # TODO\n",
    "    # 이쪽에 validation 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(val_loader)):\n",
    "        sleep(0.01)\n",
    "    \n",
    "    val_losses = None\n",
    "    train_losses = None\n",
    "    return val_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96067f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for nocall detector\n",
    "def nocall(dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    val_losses, train_losses = nocall_train(train_dataframe, val_dataframe)\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 Accuracy test 구현하시면 됩니다.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf08a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 144/144 [00:04<00:00, 29.05it/s]\n",
      "100%|███████████████████████████████████████████| 49/49 [00:01<00:00, 31.81it/s]\n"
     ]
    }
   ],
   "source": [
    "nocall(ff1010_dataframe, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8061e",
   "metadata": {},
   "source": [
    "Model 2 (Bird classificator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for mel-spectrogram images dataset\n",
    "class AudioImage(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='label_id'):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        \n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        if len(source.shape) == 3:\n",
    "            source = np.expand_dims(source, axis=2)\n",
    "        source = source.transpose(2, 0, 1, 3)\n",
    "        N, H, W, C = source.shape\n",
    "        \n",
    "        # Apply transform\n",
    "        augmented = torch.zeros(N, C, H, W).to(device)\n",
    "        for i in range(N):\n",
    "            augmented[i] = self.transform(Image.fromarray(source[i])).to(device)\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a19f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop for bird specification\n",
    "def bird_train(train_dataframe, val_dataframe):  \n",
    "    train_data = AudioImage(train_dataframe, process='train', labels='label_id')\n",
    "    val_data = AudioImage(val_dataframe, process='valid', labels='label_id')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    \n",
    "    # Test for loaders\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 training 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        sleep(0.01)\n",
    "        \n",
    "    # TODO\n",
    "    # 이쪽에 validation 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(val_loader)):\n",
    "        sleep(0.01)\n",
    "    \n",
    "    val_losses = None\n",
    "    train_losses = None\n",
    "    return val_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for bird specification\n",
    "def bird(dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    val_losses, train_losses = bird_train(train_dataframe, val_dataframe)\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 Accuracy test 구현하시면 됩니다.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird(birdclef_dataframe, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19a0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
