{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163002d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchmetrics) (21.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchmetrics) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.3.1 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torchmetrics) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ujubo\\anaconda3\\lib\\site-packages (from packaging->torchmetrics) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries for data_loader\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "#!pip install albumentations\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106e364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataLoader and corresponding libraries\n",
    "import pandas\n",
    "import torchvision.transforms as TT\n",
    "# import albumentations as T\n",
    "# import albumentations.augmentations.transforms as T_transforms\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision import utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f5e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for tensors\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# For fbeta-score\n",
    "from torchmetrics.functional import fbeta_score\n",
    "\n",
    "# For nn.Sequential\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53686be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bar construction\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "822fac7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Datatypes and Devices (from Assignment 2)\n",
    "dtype = torch.float\n",
    "ltype = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2f3e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for K-Fold Cross Validation\n",
    "N = 5\n",
    "seed = 42\n",
    "\n",
    "# Directories for Data\n",
    "FF1010_Path = 'C:\\\\users\\\\ujubo\\\\birdnocall\\\\'\n",
    "AudioImage_Path = './image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "745d6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call StratifiedKFold object\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca470f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for K-Fold Cross Validation (ff1010)\n",
    "ff1010_csv = pandas.read_csv(FF1010_Path + 'metadata.csv')\n",
    "#no call\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 0, 'filepath'] = \\\n",
    "    FF1010_Path + 'nocall/' + ff1010_csv.query('hasbird == 0')['filename'] + '.npy'\n",
    "#call\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 1, 'filepath'] = \\\n",
    "    FF1010_Path + 'bird/' + ff1010_csv.query('hasbird == 1')['filename'] + '.npy'\n",
    "\n",
    "\n",
    "ff1010_csv = ff1010_csv.dropna()\n",
    "ff1010_csv = ff1010_csv.reset_index(drop=True)\n",
    "\n",
    "# Add 'fold' attribute for dataset classification\n",
    "ff1010_dataframe = ff1010_csv.copy()\n",
    "for n, (_, nth_groups) in enumerate(\n",
    "    skf.split(ff1010_dataframe, ff1010_dataframe['hasbird'])):\n",
    "    ff1010_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d765a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify dataframe for K-Fold Cross Validation (birdclef2021)\n",
    "# birdclef_csv = pandas.read_csv(AudioImage_Path + 'metadata.csv')\n",
    "# birdclef_csv.loc[birdclef_csv['label_id'] >= 0,'filepath'] = \\\n",
    "#     AudioImage_Path + birdclef_csv.query('label_id >= 0')['primary_label'] + '/' + \\\n",
    "#     birdclef_csv.query('label_id >= 0')['filename'] + '.npy'\n",
    "\n",
    "# birdclef_csv = birdclef_csv.dropna()\n",
    "# birdclef_csv = birdclef_csv.reset_index(drop=True)\n",
    "\n",
    "# # Add 'fold' attribute for dataset classification\n",
    "# birdclef_dataframe = birdclef_csv.copy()\n",
    "# for n, (_, nth_groups) in enumerate(\n",
    "#     skf.split(birdclef_dataframe, birdclef_dataframe['label_id'])):\n",
    "#     birdclef_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "631ff9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for training \n",
    "ff1010_batch = 32\n",
    "birdclef_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "100d1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for ff1010 dataset\n",
    "class FF1010(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='hasbird'):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.train_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.RandomHorizontalFlip(p=0.5),\n",
    "            TT.RandomVerticalFlip(p=0.5),\n",
    "#             T_transforms.ImageCompression(p=0.5, \n",
    "#                 compression_type=T_transforms.ImageCompression.ImageCompressionType.JPEG),\n",
    "#             T_transforms.ImageCompression(p=0.5, \n",
    "#                 compression_type=T_transforms.ImageCompression.ImageCompressionType.WEBP),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "        self.val_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.process == 'train':\n",
    "            source = self.train_transform(Image.fromarray(source))\n",
    "        elif self.process == 'valid':\n",
    "            ource = self.val_transform(Image.fromarray(source))\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59573f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Train loop for nocall detector\n",
    "def nocall_train(train_dataframe, val_dataframe, test_dataframe):  \n",
    "    train_data = FF1010(train_dataframe, process='train', labels='hasbird')\n",
    "    val_data = FF1010(val_dataframe, process='valid', labels='hasbird')\n",
    "    test_data = FF1010(test_dataframe, process='test', labels='hasbird')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    \n",
    "    # Test for loaders\n",
    "    \n",
    "    optimizer = optim.SGD(model_1.parameters(), lr=learning_rate_1, momentum=0.5, weight_decay=weight_decay_1)\n",
    "    test_losses, val_losses, train_losses = train_model(model_1, train_loader, val_loader, test_loader, optimizer, beta=beta)\n",
    "    \n",
    "    return test_losses, val_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2857f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, beta=1.0):\n",
    "  print(\"Checking accuracy score on validation set.\")\n",
    "  # TODO: extend this so that we can print that we evaluate test set.\n",
    "  num_correct = 0\n",
    "  num_samples = 0\n",
    "  log_sum = 0\n",
    "  fbeta_sum = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "      x = x.to(device=device, dtype=torch.float)\n",
    "      y = y.to(device=device, dtype=torch.long)\n",
    "      scores = model(x)\n",
    "      # scores means classfication class for each class. It should be the tensor with size of (Input size, Number of classes)\n",
    "      # In binary classification, it should be (batch size, 2) sized tensor\n",
    "\n",
    "      # Checks naive accuracy.\n",
    "      _, preds = scores.max(1)\n",
    "      num_correct += (preds == y).sum()\n",
    "      num_samples += preds.size(0)\n",
    "\n",
    "      # Checks Log Loss.\n",
    "      # TODO: change this to F.cross_entropy()\n",
    "      log_loss = F.nll_loss(F.log_softmax(scores, dim=0), y)\n",
    "      log_sum += log_loss.sum(weight=128) * preds.size(0)\n",
    "\n",
    "      # Checks Fbeta-score.\n",
    "      fbeta = fbeta_score(preds, y, beta=beta)\n",
    "      fbeta_sum += fbeta * preds.size(0)\n",
    "\n",
    "    acc = float(num_correct) / num_samples\n",
    "    log_score = log_sum / num_samples\n",
    "    f_score = fbeta_sum / num_samples\n",
    "    print('\\nAccuracy: %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    print('Log Loss score:\\t%.2f' % (log_score))\n",
    "    print('Fbeta-score (beta=%d): \\t%.2f' % (beta , f_score))\n",
    "  return acc, log_score, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c138ea2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ujubo/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (resnet50): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=1000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print period for accuracy.\n",
    "print_period = 12\n",
    "\n",
    "# Learning Rate.\n",
    "learning_rate_1 = 0.0001\n",
    "\n",
    "# L2 Regularization Hyperparamter\n",
    "weight_decay_1 = 0.01\n",
    "\n",
    "# Beta constant for Fbeta-score.\n",
    "# If you want to give more weight to precision, use value smaller than 1.0.\n",
    "# If you want to give more weight to recall, use value larger than 1.0.\n",
    "beta = 1.0\n",
    "\n",
    "# Prototype of model 1.\n",
    "# ResNet50 outputs (Batchsize, 1000) tensor as output, so we reduce them to 2.\n",
    "# TODO: I'm curious about the output of the model: would output be the float number\n",
    "#       between 0 and 1? If not, we need to add sigmoid or softmax function at the end.\n",
    "#       (EDIT) It looks like we're ok with training.\n",
    "\n",
    "model_1 = nn.Sequential(OrderedDict([\n",
    "        (\"resnet50\", torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True).to(device)),\n",
    "        (\"relu\", nn.ReLU().to(device)),\n",
    "        (\"linear\", nn.Linear(1000, 2, bias=True).to(device))\n",
    "]))\n",
    "\n",
    "print(model_1)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, test_loader, optimizer, beta=1.0, epoch=1):\n",
    "    loss = 0\n",
    "    log_score = 0\n",
    "    test_score = 0\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        print(f\"Training model 1, epoch {e+1}\")\n",
    "        for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "            x = source.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n",
    "            y = label.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.nll_loss(F.log_softmax(scores, dim=0), y) # Log loss for our project.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # every print_period, evaluate on val_loader.\n",
    "            if index % print_period == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (index, loss.item()))\n",
    "                print()\n",
    "    \n",
    "    acc, log_score, fbeta_score = evaluate_model(val_loader, model, beta=beta)\n",
    "    acc, test_score, fbeta_score = evaluate_model(test_loader, model, beta=beta)\n",
    "\n",
    "    return test_score, log_score, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21c3c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for nocall detector\n",
    "def nocall(dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    test_dataframe = dataframe.query(\n",
    "        'fold == ' + str(test_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    test_losses, val_losses, train_losses = nocall_train(train_dataframe, val_dataframe, test_dataframe)\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 Accuracy test 구현하시면 됩니다.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b01be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1, epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 1/144 [00:03<09:19,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.6386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                         | 13/144 [00:51<08:41,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 3.7045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                   | 25/144 [01:39<08:07,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 3.6532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▊                                                            | 37/144 [02:28<07:11,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 3.5354\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████▌                                                     | 49/144 [03:16<06:16,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 3.5664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▎                                              | 61/144 [04:04<05:36,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 3.5511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████                                        | 73/144 [04:51<04:41,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 3.5338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nocall(ff1010_dataframe, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1edb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
