{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091220ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries for data_loader\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataLoader and corresponding libraries\n",
    "import pandas\n",
    "import torchvision.transforms as TT\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision import utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for tensors\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fbeta-score\n",
    "from torchmetrics.functional import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model construction\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bar construction\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40992094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# Datatypes and Devices (from Assignment 2)\n",
    "dtype = torch.float\n",
    "ltype = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for K-Fold Cross Validation\n",
    "N = 5\n",
    "seed = 42\n",
    "\n",
    "# Directories for Data\n",
    "FF1010_Path = './data/'\n",
    "AudioImage_Path = './image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38174635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call StratifiedKFold object\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for K-Fold Cross Validation (birdclef2021)\n",
    "birdclef_csv = pandas.read_csv(AudioImage_Path + 'metadata.csv')\n",
    "birdclef_csv.loc[birdclef_csv['label_id'] >= 0,'filepath'] = \\\n",
    "    AudioImage_Path + birdclef_csv.query('label_id >= 0')['primary_label'] + '/' + \\\n",
    "    birdclef_csv.query('label_id >= 0')['filename'] + '.npy'\n",
    "\n",
    "birdclef_csv = birdclef_csv.dropna()\n",
    "birdclef_csv = birdclef_csv.reset_index(drop=True)\n",
    "\n",
    "# Add 'fold' attribute for dataset classification\n",
    "birdclef_dataframe = birdclef_csv.copy()\n",
    "for n, (_, nth_groups) in enumerate(\n",
    "    skf.split(birdclef_dataframe, birdclef_dataframe['label_id'])):\n",
    "    birdclef_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db579aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for ff1010 dataset\n",
    "class birdclef(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='label_id', nocall_enabled=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        self.batch_threshold = 128\n",
    "        self.nocall_enabled = nocall_enabled\n",
    "        self.nocall_model = nn.Sequential(OrderedDict([\n",
    "            (\"resnet50\", torch.hub.load(\n",
    "                'pytorch/vision:v0.10.0', 'resnet50', pretrained=True)),\n",
    "            (\"relu\", nn.ReLU()),\n",
    "            (\"linear\", nn.Linear(1000, 2, bias=True)),\n",
    "            (\"softmax\", nn.Softmax(dim=-1))\n",
    "        ]))\n",
    "        \n",
    "        # Load checkpoint\n",
    "        if self.nocall_enabled and os.path.exists('./nocall_detector.pt'):\n",
    "            ckpt = torch.load('./nocall_detector.pt')\n",
    "            self.nocall_model.load_state_dict(ckpt)\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.train_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.RandomHorizontalFlip(p=0.5),\n",
    "            TT.RandomVerticalFlip(p=0.5),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "        self.val_transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        \n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        if len(source.shape) == 3:\n",
    "            source = np.expand_dims(source, axis=2)\n",
    "        source = source.transpose(2, 0, 1, 3)\n",
    "        N, H, W, C = source.shape\n",
    "        label_rp = N\n",
    "        \n",
    "        # Apply transform\n",
    "        nocall_source = None\n",
    "        if self.process == 'train':\n",
    "            nocall_source = torch.stack([self.val_transform(Image.fromarray(x)) for x in source])\n",
    "            source = torch.stack([self.train_transform(Image.fromarray(x)) for x in source])\n",
    "        elif self.process == 'valid':\n",
    "            source = torch.stack([self.val_transform(Image.fromarray(x)) for x in source])\n",
    "        \n",
    "        # Avoid CUDA OOM\n",
    "        if N > self.batch_threshold:\n",
    "            unit = (N - self.batch_threshold) // 2\n",
    "            label_rp = self.batch_threshold\n",
    "            if N % 2 == 0:\n",
    "                source = source[unit : -unit]\n",
    "            else:\n",
    "                source = source[unit : -(unit + 1)]\n",
    "        \n",
    "        # Enable nocall model\n",
    "        if self.nocall_enabled:\n",
    "            birdsound_list = []\n",
    "            for i in range(label_rp):\n",
    "                pred = self.nocall_model(torch.unsqueeze(nocall_source[i], 0))\n",
    "                if pred[0][1] > pred[0][0]:\n",
    "                    birdsound_list.append(source[i])\n",
    "\n",
    "            if len(birdsound_list) > 0:\n",
    "                source = torch.stack(birdsound_list)\n",
    "                label_rp = len(birdsound_list)\n",
    "            del birdsound_list\n",
    "        del nocall_source\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).repeat(label_rp)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf190da",
   "metadata": {},
   "source": [
    "This function evaluates model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, beta=1.0):\n",
    "    print(\"Checking accuracy score on validation set.\")\n",
    "    # TODO: extend this so that we can print that we evaluate test set.\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    log_sum = 0\n",
    "    fbeta_sum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            y = torch.squeeze(y, 0)\n",
    "            scores = model(torch.squeeze(x, 0))\n",
    "            # scores means classfication class for each class. It should be the tensor with size of (Input size, Number of classes)\n",
    "\n",
    "            # Checks naive accuracy.\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            # Checks Log Loss.\n",
    "            log_loss = F.cross_entropy(scores, y)\n",
    "            log_sum += log_loss.sum() * preds.size(0)\n",
    "\n",
    "            # Checks Fbeta-score.\n",
    "            fbeta = fbeta_score(preds, y, beta=beta)\n",
    "            fbeta_sum += fbeta * preds.size(0)\n",
    "            \n",
    "            # Erase memory caches\n",
    "            del x, y, scores, log_loss\n",
    "        \n",
    "        acc = float(num_correct) / num_samples\n",
    "        log_score = log_sum / num_samples\n",
    "        f_score = fbeta_sum / num_samples\n",
    "        print('\\nAccuracy: %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        print('Log Loss score:\\t%.2f' % (log_score))\n",
    "        print('Fbeta-score (beta=%d): \\t%.2f' % (beta , f_score))\n",
    "    return acc, log_score, f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9d4a5",
   "metadata": {},
   "source": [
    "Let's train with model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb37347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Print period for accuracy.\n",
    "print_period = 37724\n",
    "\n",
    "# Hyper-parameters for training \n",
    "birdclef_batch = 1\n",
    "birdclef_epoch = 10\n",
    "\n",
    "# Learning Rate.\n",
    "learning_rate_2 = 0.001\n",
    "\n",
    "# L2 Regularization Hyperparamter\n",
    "weight_decay_2 = 0.001\n",
    "\n",
    "# Beta constant for Fbeta-score.\n",
    "# If you want to give more weight to precision, use value smaller than 1.0.\n",
    "# If you want to give more weight to recall, use value larger than 1.0.\n",
    "beta = 1.0\n",
    "\n",
    "# Prototype of model 2.\n",
    "# ResNet50 outputs (Batchsize, 1000) tensor as output, so we reduce them to 397.\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, num_classes=397):\n",
    "        super().__init__()\n",
    "        self.resnet50 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "        self.linear = nn.Linear(1000, num_classes, bias=True).to(device)\n",
    "    \n",
    "    def forward (self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "cur_model = Model2()\n",
    "print(cur_model)\n",
    "\n",
    "def checkpoint(model, optimizer, loss, epoch, index):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, './bird_specificator.pt')\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, beta=beta, epoch=birdclef_epoch):\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    epoch_losses = []\n",
    "    log_scores = []\n",
    "    fbeta_scores = []\n",
    "    \n",
    "    loss = 0\n",
    "    log_score = 0\n",
    "    saved_epoch = -1\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists('./bird_specificator.pt'):\n",
    "        ckpt = torch.load('./bird_specificator.pt')\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        epoch_losses = ckpt['loss']\n",
    "        saved_epoch = ckpt['epoch']\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        if e < saved_epoch:\n",
    "            continue\n",
    "        \n",
    "        if len(val_losses) == 0 and saved_epoch > -1:\n",
    "            acc, log_score, fbeta_score = evaluate_model(val_loader, model, beta=beta)\n",
    "            val_losses.append(acc)\n",
    "            train_losses.append(torch.tensor(sum(epoch_losses) / len(epoch_losses), dtype=dtype))\n",
    "            log_scores.append(log_score)\n",
    "            fbeta_scores.append(fbeta_score)\n",
    "            \n",
    "        print(f\"Training model 2, epoch {e+1}\")\n",
    "        for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):          \n",
    "            x = source.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n",
    "            y = label.to(device=device, dtype=torch.long)\n",
    "            y = torch.squeeze(y, 0)\n",
    "\n",
    "            scores = model(torch.squeeze(x, 0))\n",
    "            loss = F.cross_entropy(scores, y) # Log loss for our project.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # every print_period, print loss.\n",
    "            if index % print_period == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (index, loss.item()))\n",
    "                \n",
    "            # Erase memory caches\n",
    "            del source, label, x, y, scores, loss\n",
    "            \n",
    "        # every epoch, save the model\n",
    "        checkpoint(model, optimizer, epoch_losses, e, index)\n",
    "        \n",
    "        # Append the score to lists\n",
    "        acc, log_score, fbeta_score = evaluate_model(val_loader, model, beta=beta)\n",
    "        val_losses.append(acc)\n",
    "        train_losses.append(torch.tensor(sum(epoch_losses) / len(epoch_losses), dtype=dtype))\n",
    "        log_scores.append(log_score)\n",
    "        fbeta_scores.append(fbeta_score)\n",
    "\n",
    "    return model, val_losses, train_losses, log_scores, fbeta_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9439959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop for bird specificator\n",
    "def bird_train(model, train_dataframe, val_dataframe, test_dataframe):  \n",
    "    train_data = birdclef(train_dataframe, process='train', labels='label_id')\n",
    "    val_data = birdclef(val_dataframe, process='valid', labels='label_id')\n",
    "    test_data = birdclef(test_dataframe, process='valid', labels='label_id')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=birdclef_batch,\n",
    "                              sampler=sampler.SubsetRandomSampler(range(len(test_dataframe))),\n",
    "                              drop_last=False)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate_2, momentum=0.5, weight_decay=weight_decay_2)\n",
    "    trained_model, val_losses, train_losses, log_scores, fbeta_scores = train_model(\n",
    "        model, train_loader, val_loader, optimizer, beta=beta)\n",
    "    test_acc, test_log_score, test_fbeta_score = evaluate_model(test_loader, trained_model, beta=beta)\n",
    "    \n",
    "    return val_losses, train_losses, log_scores, fbeta_scores, [test_acc, test_log_score, test_fbeta_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9205be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for nocall detector\n",
    "def bird(model, dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    print('Test set : %d, Validation set : %d' % (test_index, val_index))\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    test_dataframe = dataframe.query(\n",
    "        'fold == ' + str(test_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    val_losses, train_losses, log_scores, fbeta_scores, test_scores = bird_train(\n",
    "        model, train_dataframe, val_dataframe, test_dataframe)\n",
    "\n",
    "    return val_losses, train_losses, log_scores, fbeta_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses, train_losses, log_scores, fbeta_scores, test_scores = \\\n",
    "    bird(cur_model, birdclef_dataframe, 1, 0)\n",
    "del cur_model\n",
    "\n",
    "# for i in range(2, 5):\n",
    "#     cur_model = Model2()\n",
    "#     cv_losses, ctr_losses, cl_scores, cf_scores, cte_scores = nocall(cur_model, birdclef_dataframe, i, 0)\n",
    "#     val_losses = torch.mean(torch.stack((torch.tensor(val_losses), torch.tensor(cv_losses))), dim=0)\n",
    "#     train_losses = torch.mean(torch.stack((torch.tensor(train_losses), torch.tensor(ctr_losses))), dim=0)\n",
    "#     log_scores = torch.mean(torch.stack((torch.tensor(log_scores), torch.tensor(cl_scores))), dim=0)\n",
    "#     fbeta_scores = torch.mean(torch.stack((torch.tensor(fbeta_scores), torch.tensor(cf_scores))), dim=0)\n",
    "#     for j in range(len(test_scores)):\n",
    "#         test_scores[j] = torch.mean(torch.stack((torch.tensor(test_scores[j]), torch.tensor(cte_scores[j]))), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = torch.tensor(train_losses, device = 'cpu')\n",
    "plt.plot(np.arange(len(train_losses)), train_losses)\n",
    "plt.title('Train Loss Curve of Nocall Detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = torch.tensor(val_losses, device = 'cpu')\n",
    "plt.plot(np.arange(len(val_losses)), val_losses)\n",
    "plt.title('Validation Accuracy Curve of Nocall Detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores = torch.tensor(log_scores, device = 'cpu')\n",
    "plt.plot(np.arange(len(log_scores)), log_scores)\n",
    "plt.title('Log-score Curve of Nocall Detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_scores = torch.tensor(fbeta_scores, device = 'cpu')\n",
    "plt.plot(np.arange(len(fbeta_scores)), fbeta_scores)\n",
    "plt.title('F-beta score Curve of Nocall Detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2163b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy: %.2f' % (100 * test_scores[0]))\n",
    "print('Log Loss score:\\t%.2f' % (test_scores[1]))\n",
    "print('Fbeta-score (beta=1.0): \\t%.2f' % (test_scores[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
