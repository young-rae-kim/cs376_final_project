{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db099f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: albumentations in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (1.7.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (4.5.5.64)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from albumentations) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (0.24.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.4.3)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (2.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\youngrae\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries for data_loader\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install albumentations\n",
    "!pip install torchvision\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307d1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataLoader and corresponding libraries\n",
    "import pandas\n",
    "import torchvision.transforms as TT\n",
    "import albumentations as T\n",
    "import albumentations.augmentations.transforms as T_transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision import utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88dc1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for tensors\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824ee728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bar construction\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18c7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Datatypes and Devices (from Assignment 2)\n",
    "dtype = torch.float\n",
    "ltype = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cde023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for K-Fold Cross Validation\n",
    "N = 5\n",
    "seed = 42\n",
    "\n",
    "# Directories for Data\n",
    "FF1010_Path = './data/'\n",
    "AudioImage_Path = './image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ff2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call StratifiedKFold object\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5, shuffle=True, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3a6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for K-Fold Cross Validation (ff1010)\n",
    "ff1010_csv = pandas.read_csv(FF1010_Path + 'metadata.csv')\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 0, 'filepath'] = \\\n",
    "    FF1010_Path + 'nocall/' + ff1010_csv.query('hasbird == 0')['filename'] + '.npy'\n",
    "ff1010_csv.loc[ff1010_csv['hasbird'] == 1, 'filepath'] = \\\n",
    "    FF1010_Path + 'bird/' + ff1010_csv.query('hasbird == 1')['filename'] + '.npy'\n",
    "\n",
    "ff1010_csv = ff1010_csv.dropna()\n",
    "ff1010_csv = ff1010_csv.reset_index(drop=True)\n",
    "\n",
    "# Add 'fold' attribute for dataset classification\n",
    "ff1010_dataframe = ff1010_csv.copy()\n",
    "for n, (_, nth_groups) in enumerate(\n",
    "    skf.split(ff1010_dataframe, ff1010_dataframe['hasbird'])):\n",
    "    ff1010_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b45549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe for K-Fold Cross Validation (birdclef2021)\n",
    "birdclef_csv = pandas.read_csv(AudioImage_Path + 'metadata.csv')\n",
    "birdclef_csv.loc[birdclef_csv['label_id'] >= 0,'filepath'] = \\\n",
    "    AudioImage_Path + birdclef_csv.query('label_id >= 0')['primary_label'] + '/' + \\\n",
    "    birdclef_csv.query('label_id >= 0')['filename'] + '.npy'\n",
    "\n",
    "birdclef_csv = birdclef_csv.dropna()\n",
    "birdclef_csv = birdclef_csv.reset_index(drop=True)\n",
    "\n",
    "# Add 'fold' attribute for dataset classification\n",
    "birdclef_dataframe = birdclef_csv.copy()\n",
    "for n, (_, nth_groups) in enumerate(\n",
    "    skf.split(birdclef_dataframe, birdclef_dataframe['label_id'])):\n",
    "    birdclef_dataframe.loc[nth_groups, 'fold'] = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d909d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for training \n",
    "ff1010_batch = 32\n",
    "birdclef_batch = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5213b97",
   "metadata": {},
   "source": [
    "Model 1 (No-call detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for ff1010 dataset\n",
    "class FF1010(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='hasbird'):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.train_transform = T.Compose([\n",
    "            T.Resize(128, 281),\n",
    "            T.HorizontalFlip(p=0.5),\n",
    "            T.VerticalFlip(p=0.5),\n",
    "            T_transforms.ImageCompression(p=0.5, \n",
    "                compression_type=T_transforms.ImageCompression.ImageCompressionType.JPEG),\n",
    "            T_transforms.ImageCompression(p=0.5, \n",
    "                compression_type=T_transforms.ImageCompression.ImageCompressionType.WEBP),\n",
    "            T.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.val_transform = T.Compose([\n",
    "            T.Resize(128, 281),\n",
    "            T.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.process == 'train':\n",
    "            transformed = self.train_transform(image=source)\n",
    "            source = transformed['image'].to(device)\n",
    "        elif self.process == 'valid':\n",
    "            transformed = self.val_transform(image=source)\n",
    "            source = transformed['image'].to(device)\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c650ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Train loop for nocall detector\n",
    "def nocall_train(train_dataframe, val_dataframe):  \n",
    "    train_data = FF1010(train_dataframe, process='train', labels='hasbird')\n",
    "    val_data = FF1010(val_dataframe, process='valid', labels='hasbird')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=ff1010_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    \n",
    "    # Test for loaders\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 training 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        sleep(0.01)\n",
    "        \n",
    "    # TODO\n",
    "    # 이쪽에 validation 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(val_loader)):\n",
    "        sleep(0.01)\n",
    "    \n",
    "    val_losses = None\n",
    "    train_losses = None\n",
    "    return val_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96067f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for nocall detector\n",
    "def nocall(dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    val_losses, train_losses = nocall_train(train_dataframe, val_dataframe)\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 Accuracy test 구현하시면 됩니다.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf08a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 144/144 [00:22<00:00,  6.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49/49 [00:02<00:00, 21.64it/s]\n"
     ]
    }
   ],
   "source": [
    "nocall(ff1010_dataframe, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8061e",
   "metadata": {},
   "source": [
    "Model 2 (Bird classificator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b3c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for mel-spectrogram images dataset\n",
    "class AudioImage(Dataset):\n",
    "    def __init__(self, dataframe, process='train', labels='label_id'):\n",
    "        self.dataframe = dataframe\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.labels = dataframe[labels].values\n",
    "        self.process = process\n",
    "        \n",
    "        # Transforms for each train and validation\n",
    "        self.transform = TT.Compose([\n",
    "            TT.Resize([128, 281]),\n",
    "            TT.ToTensor(),\n",
    "            TT.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = np.load(self.filepaths[idx])\n",
    "        \n",
    "        # Rearrange numpy arrays\n",
    "        source = source.transpose(1, 2, 0)\n",
    "        \n",
    "        # Add RGB dimension\n",
    "        source = np.stack((np.squeeze(source), ) * 3, -1)\n",
    "        if len(source.shape) == 3:\n",
    "            source = np.expand_dims(source, axis=2)\n",
    "        source = source.transpose(2, 0, 1, 3)\n",
    "        N, H, W, C = source.shape\n",
    "        \n",
    "        # Apply transform\n",
    "        augmented = torch.zeros(N, C, H, W).to(device)\n",
    "        for i in range(N):\n",
    "            augmented[i] = self.transform(Image.fromarray(source[i])).to(device)\n",
    "        \n",
    "        return source, torch.tensor(self.labels[idx], dtype=ltype).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a19f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop for bird specification\n",
    "def bird_train(train_dataframe, val_dataframe):  \n",
    "    train_data = AudioImage(train_dataframe, process='train', labels='label_id')\n",
    "    val_data = AudioImage(val_dataframe, process='valid', labels='label_id')\n",
    "    \n",
    "    # Construct data loader for train and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(train_dataframe))), \n",
    "                             drop_last=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=birdclef_batch,\n",
    "                             sampler=sampler.SubsetRandomSampler(range(len(val_dataframe))),\n",
    "                             drop_last=False)\n",
    "    \n",
    "    # Test for loaders\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 training 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        sleep(0.01)\n",
    "        \n",
    "    # TODO\n",
    "    # 이쪽에 validation 구현하시면 됩니다.\n",
    "    for index, (source, label) in enumerate(tqdm.tqdm(val_loader)):\n",
    "        sleep(0.01)\n",
    "    \n",
    "    val_losses = None\n",
    "    train_losses = None\n",
    "    return val_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2130907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate and Test for bird specification\n",
    "def bird(dataframe, val_index, test_index):\n",
    "    \n",
    "    # Check that validation fold is not same as test fold\n",
    "    assert val_index != test_index, \\\n",
    "        'Validation and test should be done on different fold.'\n",
    "    \n",
    "    train_dataframe = dataframe.query(\n",
    "        'fold != ' + str(val_index) + ' and fold != ' + str(test_index) \n",
    "    ).reset_index(drop=True)\n",
    "    val_dataframe = dataframe.query(\n",
    "        'fold == ' + str(val_index) \n",
    "    ).reset_index(drop=False)\n",
    "    \n",
    "    val_losses, train_losses = bird_train(train_dataframe, val_dataframe)\n",
    "    \n",
    "    # TODO\n",
    "    # 이쪽에 Accuracy test 구현하시면 됩니다.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f9f21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/294 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6, 128, 281, 3] at entry 0 and [7, 128, 281, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17108/909086868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbird\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbirdclef_dataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17108/3848076975.py\u001b[0m in \u001b[0;36mbird\u001b[1;34m(dataframe, val_index, test_index)\u001b[0m\n\u001b[0;32m     13\u001b[0m     ).reset_index(drop=False)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbird_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17108/1878293250.py\u001b[0m in \u001b[0;36mbird_train\u001b[1;34m(train_dataframe, val_dataframe)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# 이쪽에 training 구현하시면 됩니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6, 128, 281, 3] at entry 0 and [7, 128, 281, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "bird(birdclef_dataframe, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19a0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
